{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "training_data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the first five elements of the DataFrame\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\r\\n\\r\\nThat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\r\\n\\r\\nUmm, theres no actual article ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\r\\nAnd ... I really don't think you understa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\r\\n\\r\\nThat...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\r\\n\\r\\nUmm, theres no actual article ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\r\\nAnd ... I really don't think you understa...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the last five elements of the DataFrame\n",
    "training_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the row \"toxic\": [0 1]\n",
      "Unique values in the row \"severe_toxic\": [0 1]\n",
      "Unique values in the row \"obscene\": [0 1]\n",
      "Unique values in the row \"threat\": [0 1]\n",
      "Unique values in the row \"insult\": [0 1]\n",
      "Unique values in the row \"indentity_hate\": [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Showing all unique values in the classification rows\n",
    "print(f\"Unique values in the row \\\"toxic\\\": {training_data['toxic'].unique()}\")\n",
    "print(f\"Unique values in the row \\\"severe_toxic\\\": {training_data['severe_toxic'].unique()}\")\n",
    "print(f\"Unique values in the row \\\"obscene\\\": {training_data['obscene'].unique()}\")\n",
    "print(f\"Unique values in the row \\\"threat\\\": {training_data['threat'].unique()}\")\n",
    "print(f\"Unique values in the row \\\"insult\\\": {training_data['insult'].unique()}\")\n",
    "print(f\"Unique values in the row \\\"indentity_hate\\\": {training_data['identity_hate'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tompr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tompr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tompr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"Proprocessing the comments of the raw data\"\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # removing usernames starting with '@'\n",
    "    text = re.sub(r'@ ?\\w+', '', text)\n",
    "    # removing URL's\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    # removing the substring \"quot\", which is an HTML entity for double quotation (\"\")\n",
    "    text = re.sub(r'&quot', '', text)\n",
    "    # removing all special characters\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # removing all digits\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    # Join the tokens back into a single string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies the preprocessing function to all comments \n",
    "training_data['comment_text'] = training_data['comment_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                                       comment_text  \\\n",
      "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
      "1       000103f0d9cfb60f  aww match background colour seemingly stuck th...   \n",
      "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
      "3       0001b41b1c6bb37e  make real suggestion improvement wondered sect...   \n",
      "4       0001d958c54c6e35                      sir hero chance remember page   \n",
      "...                  ...                                                ...   \n",
      "159566  ffe987279560d7ff  second time asking view completely contradicts...   \n",
      "159567  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
      "159568  ffee36eab5c267c9  spitzer umm there actual article prostitution ...   \n",
      "159569  fff125370e4aaaf3  look like actually put speedy first version de...   \n",
      "159570  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
      "\n",
      "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0           0             0        0       0       0              0  \n",
      "1           0             0        0       0       0              0  \n",
      "2           0             0        0       0       0              0  \n",
      "3           0             0        0       0       0              0  \n",
      "4           0             0        0       0       0              0  \n",
      "...       ...           ...      ...     ...     ...            ...  \n",
      "159566      0             0        0       0       0              0  \n",
      "159567      0             0        0       0       0              0  \n",
      "159568      0             0        0       0       0              0  \n",
      "159569      0             0        0       0       0              0  \n",
      "159570      0             0        0       0       0              0  \n",
      "\n",
      "[159571 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tompr\\desktop\\mlfmde\\toxic-comments-classifier\\ml-proj\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "def create_tfidf_model(data : pd.core.frame.DataFrame)->None:\n",
    "    \"Creates the tf-idf model and saves it\"\n",
    "    vectorizer = TfidfVectorizer(max_features=10000, min_df=5, max_df=0.8)\n",
    "    tfidf_matrix = vectorizer.fit_transform(data[\"comment_text\"].to_list())\n",
    "\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    with open('TF-IDF/tfidf_vectorizer.pkl', 'wb') as file:\n",
    "        pickle.dump(vectorizer, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec_model(training_data : pd.core.frame.DataFrame)->None:\n",
    "    \"Trains the word2vec model and saves it\"\n",
    "    # converting datafram column into a list of strings\n",
    "    tokenized_sentences = []\n",
    "    for sentence in training_data[\"comment_text\"].to_list():\n",
    "        tokenized_sentences.append(sentence.split())\n",
    "\n",
    "    # train word2vec model\n",
    "    word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # save model\n",
    "    word2vec_model.save(\"word2vec_model.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(\"word2vec/word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word2vec(sentence : str)->list:\n",
    "    \"uses word2vec to vectorize a sentence\"\n",
    "    word_vectors = [word2vec_model.wv[word] for word in sentence.split() if word in word2vec_model.wv]\n",
    "    return word_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                                       comment_text  \\\n",
      "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
      "1       000103f0d9cfb60f  aww match background colour seemingly stuck th...   \n",
      "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
      "3       0001b41b1c6bb37e  make real suggestion improvement wondered sect...   \n",
      "4       0001d958c54c6e35                      sir hero chance remember page   \n",
      "...                  ...                                                ...   \n",
      "159566  ffe987279560d7ff  second time asking view completely contradicts...   \n",
      "159567  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
      "159568  ffee36eab5c267c9  spitzer umm there actual article prostitution ...   \n",
      "159569  fff125370e4aaaf3  look like actually put speedy first version de...   \n",
      "159570  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
      "\n",
      "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0           0             0        0       0       0              0  \n",
      "1           0             0        0       0       0              0  \n",
      "2           0             0        0       0       0              0  \n",
      "3           0             0        0       0       0              0  \n",
      "4           0             0        0       0       0              0  \n",
      "...       ...           ...      ...     ...     ...            ...  \n",
      "159566      0             0        0       0       0              0  \n",
      "159567      0             0        0       0       0              0  \n",
      "159568      0             0        0       0       0              0  \n",
      "159569      0             0        0       0       0              0  \n",
      "159570      0             0        0       0       0              0  \n",
      "\n",
      "[159571 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# APPLY WORD2VEC\n",
    "training_data['comment_text'] = training_data['comment_text'].apply(apply_word2vec)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         __  _noticeboard   aa  aaron   ab  abandon  abandoned  abbey  \\\n",
      "0       0.0           0.0  0.0    0.0  0.0      0.0        0.0    0.0   \n",
      "1       0.0           0.0  0.0    0.0  0.0      0.0        0.0    0.0   \n",
      "2       0.0           0.0  0.0    0.0  0.0      0.0        0.0    0.0   \n",
      "3       0.0           0.0  0.0    0.0  0.0      0.0        0.0    0.0   \n",
      "4       0.0           0.0  0.0    0.0  0.0      0.0        0.0    0.0   \n",
      "...     ...           ...  ...    ...  ...      ...        ...    ...   \n",
      "159566  0.0           0.0  0.0    0.0  0.0      0.0        0.0    0.0   \n",
      "159567  0.0           0.0  0.0    0.0  0.0      0.0        0.0    0.0   \n",
      "159568  0.0           0.0  0.0    0.0  0.0      0.0        0.0    0.0   \n",
      "159569  0.0           0.0  0.0    0.0  0.0      0.0        0.0    0.0   \n",
      "159570  0.0           0.0  0.0    0.0  0.0      0.0        0.0    0.0   \n",
      "\n",
      "        abbreviation  abc  ...  zero  zeus  zinc  zionism  zionist  zoe  \\\n",
      "0                0.0  0.0  ...   0.0   0.0   0.0      0.0      0.0  0.0   \n",
      "1                0.0  0.0  ...   0.0   0.0   0.0      0.0      0.0  0.0   \n",
      "2                0.0  0.0  ...   0.0   0.0   0.0      0.0      0.0  0.0   \n",
      "3                0.0  0.0  ...   0.0   0.0   0.0      0.0      0.0  0.0   \n",
      "4                0.0  0.0  ...   0.0   0.0   0.0      0.0      0.0  0.0   \n",
      "...              ...  ...  ...   ...   ...   ...      ...      ...  ...   \n",
      "159566           0.0  0.0  ...   0.0   0.0   0.0      0.0      0.0  0.0   \n",
      "159567           0.0  0.0  ...   0.0   0.0   0.0      0.0      0.0  0.0   \n",
      "159568           0.0  0.0  ...   0.0   0.0   0.0      0.0      0.0  0.0   \n",
      "159569           0.0  0.0  ...   0.0   0.0   0.0      0.0      0.0  0.0   \n",
      "159570           0.0  0.0  ...   0.0   0.0   0.0      0.0      0.0  0.0   \n",
      "\n",
      "        zombie  zone  zoo  zuck  \n",
      "0          0.0   0.0  0.0   0.0  \n",
      "1          0.0   0.0  0.0   0.0  \n",
      "2          0.0   0.0  0.0   0.0  \n",
      "3          0.0   0.0  0.0   0.0  \n",
      "4          0.0   0.0  0.0   0.0  \n",
      "...        ...   ...  ...   ...  \n",
      "159566     0.0   0.0  0.0   0.0  \n",
      "159567     0.0   0.0  0.0   0.0  \n",
      "159568     0.0   0.0  0.0   0.0  \n",
      "159569     0.0   0.0  0.0   0.0  \n",
      "159570     0.0   0.0  0.0   0.0  \n",
      "\n",
      "[159571 rows x 10000 columns]\n"
     ]
    }
   ],
   "source": [
    "create_tfidf_model(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY TF-IDF\n",
    "with open('TF-IDF/tfidf_vectorizer.pkl', 'rb') as file:\n",
    "    loaded_vectorizer_pickle = pickle.load(file)\n",
    "new_X = loaded_vectorizer_pickle.transform(training_data[\"comment_text\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1607)\t0.30405713848074467\n",
      "  (0, 2879)\t0.14531612893393472\n",
      "  (0, 3241)\t0.19893795086719904\n",
      "  (0, 3287)\t0.2654554271852859\n",
      "  (0, 3336)\t0.2141972155976803\n",
      "  (0, 3722)\t0.2698575976776164\n",
      "  (0, 4020)\t0.3005751191711747\n",
      "  (0, 5331)\t0.14208892676056953\n",
      "  (0, 5576)\t0.3450061195119402\n",
      "  (0, 5946)\t0.14286435579357948\n",
      "  (0, 6341)\t0.09365630563113972\n",
      "  (0, 6648)\t0.10625846190140026\n",
      "  (0, 7412)\t0.16222106807714726\n",
      "  (0, 7544)\t0.2727849817452694\n",
      "  (0, 7571)\t0.17595865365306546\n",
      "  (0, 8153)\t0.14528735663500886\n",
      "  (0, 8795)\t0.09926187397041963\n",
      "  (0, 8865)\t0.17886520760860894\n",
      "  (0, 9454)\t0.2093687767408963\n",
      "  (0, 9498)\t0.16306213262942273\n",
      "  (0, 9633)\t0.263835867216898\n",
      "  (0, 9971)\t0.23015813010457778\n",
      "  (1, 745)\t0.3370734626981767\n",
      "  (1, 1686)\t0.42481028742626853\n",
      "  (1, 4775)\t0.33668267906662697\n",
      "  :\t:\n",
      "  (159568, 7637)\t0.38593865220713075\n",
      "  (159568, 8941)\t0.4144124651110586\n",
      "  (159568, 9271)\t0.43921309775884115\n",
      "  (159569, 109)\t0.3169511991333879\n",
      "  (159569, 2359)\t0.30405550241427015\n",
      "  (159569, 3469)\t0.280149307029425\n",
      "  (159569, 5176)\t0.2168162151128002\n",
      "  (159569, 5266)\t0.5557020553494773\n",
      "  (159569, 7074)\t0.3139211949631474\n",
      "  (159569, 8380)\t0.37797582197573576\n",
      "  (159569, 9552)\t0.36246692330425023\n",
      "  (159570, 722)\t0.4216028139700269\n",
      "  (159570, 754)\t0.4026810502461747\n",
      "  (159570, 1256)\t0.21681191636841363\n",
      "  (159570, 1737)\t0.2139266721264205\n",
      "  (159570, 3828)\t0.3087070019592986\n",
      "  (159570, 4105)\t0.26107242599728725\n",
      "  (159570, 4311)\t0.38562865140712843\n",
      "  (159570, 4585)\t0.1945980308440474\n",
      "  (159570, 4948)\t0.2003166901053046\n",
      "  (159570, 7228)\t0.16040502308864818\n",
      "  (159570, 7594)\t0.27361694924221763\n",
      "  (159570, 7631)\t0.15968222727631598\n",
      "  (159570, 8955)\t0.13076632562338394\n",
      "  (159570, 9309)\t0.18430764369688013\n"
     ]
    }
   ],
   "source": [
    "print(new_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
